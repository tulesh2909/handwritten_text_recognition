{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tules\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel,Trainer, TrainingArguments, TrOCRForCausalLM\n",
    "from PIL import Image\n",
    "import torch\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tkinter import filedialog\n",
    "import fitz\n",
    "import os\n",
    "from natsort import natsorted\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #\"cuda\" if torch.cuda.is_available() else "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_from_folder(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    image_files = [os.path.join(folder_path, file) for file in files if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    return image_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr(image, processor, model):\n",
    "    image = Image.open(image).convert('RGB')\n",
    "    pixel_values = processor(image, return_tensors='pt').pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# import pandas as pd\n",
    "# # df = pd.read_csv(r'D:\\MAIN\\MEMORY_STACK\\scanToText\\TROCR\\formData copy.csv', header=None, names=[\"image_path\", \"text\"])\n",
    "# df = pd.read_csv(r'D:\\MAIN\\MEMORY_STACK\\scanToText\\TROCR\\final_dataset.csv', header=None, names=[\"image_path\", \"text\"])\n",
    "\n",
    "# # Convert the DataFrame into a Hugging Face Dataset\n",
    "# dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# print(df.columns)  # Ensure \"image_path\" and \"text\" or their equivalents exist\n",
    "# print(df.head())   # Inspect a few rows to confirm structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define your preprocessing function\n",
    "# def preprocess_function(examples):\n",
    "#     # Load images from the \"image_path\" column\n",
    "#     images = [Image.open(path).convert(\"RGB\") for path in examples[\"image_path\"]]\n",
    "    \n",
    "#     # Convert images to pixel values using the processor\n",
    "#     pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "#     print(pixel_values.shape)\n",
    "#     # Tokenize the text labels\n",
    "#     labels = processor.tokenizer(examples[\"text\"], return_tensors=\"pt\",max_length=7, padding=True, truncation=True).input_ids\n",
    "    \n",
    "#     return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# # Apply the preprocessing function to the dataset\n",
    "# # train_dataset = dataset.shuffle(seed=42).map(preprocess_function, batched=True)\n",
    "# train_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "# model.config.pad_token_id = processor.tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # training_args = TrainingArguments(\n",
    "# #     output_dir=\"./results\",\n",
    "# #     learning_rate=2e-5,\n",
    "# #     per_device_train_batch_size=16,\n",
    "# #     num_train_epochs=3,\n",
    "# # )\n",
    "\n",
    "\n",
    "# # from transformers import Seq2SeqTrainingArguments,Seq2SeqTrainer\n",
    "\n",
    "# # training_args = Seq2SeqTrainingArguments(\n",
    "# #     output_dir=\"./results\",\n",
    "# #     eval_strategy=\"no\",\n",
    "# #     per_device_train_batch_size=8,\n",
    "# #     per_device_eval_batch_size=8,\n",
    "# #     gradient_accumulation_steps= 4,\n",
    "# #     predict_with_generate=True,\n",
    "# #     logging_steps=10,\n",
    "# #     num_train_epochs=3,\n",
    "# #     fp16=True,\n",
    "# #     use_cpu=True\n",
    "# # )\n",
    "\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     # per_device_eval_batch_size=8,\n",
    "#     evaluation_strategy=\"no\",\n",
    "#     num_train_epochs=3,\n",
    "#     save_steps=500,\n",
    "#     # eval_steps=500,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     learning_rate=5e-5\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # trainer = Trainer(\n",
    "# #     model=model,\n",
    "# #     args=training_args,\n",
    "# #     train_dataset=dataset\n",
    "# # )\n",
    "\n",
    "# # trainer = Seq2SeqTrainer(\n",
    "# #     model=model.to(device),\n",
    "# #     args=training_args,\n",
    "# #     train_dataset= train_dataset\n",
    "# # )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,  # Your training dataset\n",
    "#     # eval_dataset=eval_dataset     # Your evaluation dataset\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"TROCR_final_dataset0/fine-tuned-model\")\n",
    "# processor.save_pretrained(\"TROCR_final_dataset0/fine-tuned-processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"TROCR_final_dataset0/fine-tuned-model\").to(device)\n",
    "processor = TrOCRProcessor.from_pretrained(\"TROCR_final_dataset0/fine-tuned-processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filename=filedialog.askopenfilename(initialdir=r'D:\\MAIN\\MEMORY_STACK\\scanToText\\images\\random\\test\\test_new',title=\"select a file\",filetypes=((\"pdf files\",\"*.jpeg *.png *.jpg *.pdf\"),(\"All files\", \"*.*\"))) #select the file \n",
    "\n",
    "def pdf_to_images(pdf_path, output_folder): #convert pdf to image\n",
    "    document = fitz.open(pdf_path)\n",
    "    for page_number in range(len(document)): #select each page of the pdf\n",
    "        page = document.load_page(page_number)\n",
    "        pix = page.get_pixmap()\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        image_name = f\"{output_folder}/data{page_number + 1}.jpg\"\n",
    "        img.save(image_name)\n",
    "\n",
    "    document.close()\n",
    "\n",
    "    return image_name\n",
    "\n",
    "def preprocess_image(filename):\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        filename= (pdf_to_images(filename,r'D:\\handwritten_recognition\\pdf_to_image'))\n",
    "    elif filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        filename= (filename)\n",
    "    else:\n",
    "        print(\"Unsupported file type.\")\n",
    "    return filename\n",
    "\n",
    "image_path = preprocess_image(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to delete all files in a folder\n",
    "def clear_folder(folder_path):\n",
    "    files = glob.glob(os.path.join(folder_path, '*')) # select each file from the folder \n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file) # removing all files before appending cropped cells in the folder\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {e}\")\n",
    "\n",
    "# Sort contours by row\n",
    "def sort_contours_by_row(cnts, threshold=10):\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in cnts] # use the bounding box detected by openCV\n",
    "    cnts_boundingBoxes = sorted(zip(cnts, boundingBoxes), key=lambda b: b[1][1])  # Sort by y-coordinate\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = cnts_boundingBoxes[0][1][1]  # Initial y-coordinate of the first bounding box\n",
    "    \n",
    "    for contour, bbox in cnts_boundingBoxes:\n",
    "        x, y, w, h = bbox\n",
    "        if abs(y - prev_y) <= threshold:\n",
    "            current_row.append((contour, bbox))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(contour, bbox)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    sorted_cnts = []\n",
    "    sorted_boundingBoxes = []\n",
    "    for row in rows:\n",
    "        row_sorted = sorted(row, key=lambda b: b[1][0])  # Sort by x-coordinate\n",
    "        sorted_cnts.extend([c[0] for c in row_sorted])\n",
    "        sorted_boundingBoxes.extend([c[1] for c in row_sorted])\n",
    "    return sorted_cnts, sorted_boundingBoxes\n",
    "\n",
    "# Function to rotate the image based on detected lines\n",
    "def rotate_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi/180, 200)\n",
    "\n",
    "    angles = []\n",
    "    for line in lines:\n",
    "        for rho, theta in line:\n",
    "            angle = np.degrees(theta) - 90  # Convert radians to degrees and adjust the angle\n",
    "            angles.append(angle)\n",
    "    \n",
    "    median_angle = np.median(angles)  # Compute median angle\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, median_angle, 1.0)\n",
    "    rotated_image = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    \n",
    "    return rotated_image\n",
    "\n",
    "# detect the table and break the table into cells\n",
    "def table_detection(img_path,folder_path):\n",
    "    clear_folder(folder_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = rotate_image(img)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    (_, thresh) = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "\n",
    "    kernel_length_v = (np.array(gray).shape[1])//60\n",
    "    kernel_length_h = (np.array(gray).shape[1])//50\n",
    "    \n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,kernel_length_v )) \n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_length_h, 1))\n",
    "    \n",
    "    im_temp1 = cv2.erode(thresh, vertical_kernel, iterations=3)\n",
    "    im_temp2 = cv2.erode(thresh, horizontal_kernel, iterations=3)\n",
    "\n",
    "    vertical_lines_img = cv2.dilate(im_temp1, vertical_kernel, iterations=3) # detect the vertical lines \n",
    "    horizontal_lines_img = cv2.dilate(im_temp2, horizontal_kernel, iterations=3) # detect the horizontal lines \n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    table_segment = cv2.addWeighted(vertical_lines_img, 0.5, horizontal_lines_img, 0.5, 0.0) # use the detected lines to form a table \n",
    "    table_segment = cv2.erode(cv2.bitwise_not(table_segment), kernel, iterations=2)\n",
    "    _, table_segment = cv2.threshold(table_segment, 0, 255, cv2.THRESH_OTSU)\n",
    "   \n",
    "    contours, hierarchy = cv2.findContours(table_segment, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # detect different cells of the detected table\n",
    "    contours = sort_contours_by_row(contours)[0] \n",
    "    count = 0\n",
    "    for c in contours:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        if w > h and w > 15 and h > 10:\n",
    "            count += 1\n",
    "            cropped = img[y-2:y + h +5, x-2:x + w+3] #crop the detected cell and save it\n",
    "            cell_filename = f'cell_{count}.png'\n",
    "            cv2.imwrite(f'cropped_cells/{cell_filename}', cropped)\n",
    "            cv2.rectangle(img,(x, y),(x + w, y + h),(0, 255, 0))\n",
    "\n",
    "    if True:    \n",
    "        cv2.imwrite(\"table.jpg\", img)\n",
    "        cv2.imwrite(\"table_detect.jpg\", table_segment)\n",
    "        \n",
    "folder_path = r'D:\\handwritten_recognition\\cropped_cells' # folder where cropped cell with be saved\n",
    "table_detection(image_path, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = cv2.imread(image) # load image using OpenCV\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (1, 1), 0)\n",
    "    _, thresholded_image = cv2.threshold(blurred_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    denoised_image = cv2.fastNlMeansDenoising(thresholded_image, None, 30, 7, 21)\n",
    "    processed_image_path = 'processed_image.jpg'\n",
    "    cv2.imwrite(processed_image_path, thresholded_image)\n",
    "    return processed_image_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use cropped cells for handwritten text detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tules\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cell_images = get_images_from_folder(folder_path)\n",
    "detected_text = {}\n",
    "for idx, cell_path in enumerate(cell_images):\n",
    "    processed_cell_path = preprocess_image(cell_path) # preprocessing each image \n",
    "    text = ocr(processed_cell_path, processor, model) # using trocr model to predict handwritten text\n",
    "    detected_text[(idx)] = {'filename': os.path.basename(cell_path),'text': text} # add predicted text to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({0: {'filename': 'cell_1.png', 'text': 'S.No.'}, 101: {'filename': 'cell_2.png', 'text': 'Plate ID'}, 112: {'filename': 'cell_3.png', 'text': 'Thick'}, 123: {'filename': 'cell_4.png', 'text': 'Width'}, 134: {'filename': 'cell_5.png', 'text': 'Length'}, 145: {'filename': 'cell_6.png', 'text': 'Unique ID'}, 156: {'filename': 'cell_7.png', 'text': 'Quality Remark'}, 167: {'filename': 'cell_8.png', 'text': '1'}, 178: {'filename': 'cell_9.png', 'text': '3419411ACC'}, 1: {'filename': 'cell_10.png', 'text': '10'}, 12: {'filename': 'cell_11.png', 'text': '2000'}, 23: {'filename': 'cell_12.png', 'text': '12000'}, 34: {'filename': 'cell_13.png', 'text': 'E250BR'}, 45: {'filename': 'cell_14.png', 'text': '04R'}, 56: {'filename': 'cell_15.png', 'text': '2'}, 67: {'filename': 'cell_16.png', 'text': '3419411ACE'}, 78: {'filename': 'cell_17.png', 'text': '10'}, 89: {'filename': 'cell_18.png', 'text': '2000'}, 100: {'filename': 'cell_19.png', 'text': '12000'}, 102: {'filename': 'cell_20.png', 'text': 'text'}, 103: {'filename': 'cell_21.png', 'text': '04'}, 104: {'filename': 'cell_22.png', 'text': '3'}, 105: {'filename': 'cell_23.png', 'text': '3419409CCB'}, 106: {'filename': 'cell_24.png', 'text': '10'}, 107: {'filename': 'cell_25.png', 'text': '2000'}, 108: {'filename': 'cell_26.png', 'text': '12000'}, 109: {'filename': 'cell_27.png', 'text': 'text'}, 110: {'filename': 'cell_28.png', 'text': '04k'}, 111: {'filename': 'cell_29.png', 'text': '4'}, 113: {'filename': 'cell_30.png', 'text': '3419409CCC'}, 114: {'filename': 'cell_31.png', 'text': '10'}, 115: {'filename': 'cell_32.png', 'text': '2000'}, 116: {'filename': 'cell_33.png', 'text': '12000'}, 117: {'filename': 'cell_34.png', 'text': 'text'}, 118: {'filename': 'cell_35.png', 'text': '04R'}, 119: {'filename': 'cell_36.png', 'text': '5'}, 120: {'filename': 'cell_37.png', 'text': '3419432DAO'}, 121: {'filename': 'cell_38.png', 'text': '22'}, 122: {'filename': 'cell_39.png', 'text': '2000'}, 124: {'filename': 'cell_40.png', 'text': '12000'}, 125: {'filename': 'cell_41.png', 'text': 'text'}, 126: {'filename': 'cell_42.png', 'text': '01pcs'}, 127: {'filename': 'cell_43.png', 'text': '6'}, 128: {'filename': 'cell_44.png', 'text': 'text'}, 129: {'filename': 'cell_45.png', 'text': 'text'}, 130: {'filename': 'cell_46.png', 'text': 'text'}, 131: {'filename': 'cell_47.png', 'text': 'text'}, 132: {'filename': 'cell_48.png', 'text': 'text'}, 133: {'filename': 'cell_49.png', 'text': 'text'}, 135: {'filename': 'cell_50.png', 'text': '7'}, 136: {'filename': 'cell_51.png', 'text': 'text'}, 137: {'filename': 'cell_52.png', 'text': 'text'}, 138: {'filename': 'cell_53.png', 'text': '1'}, 139: {'filename': 'cell_54.png', 'text': 'text'}, 140: {'filename': 'cell_55.png', 'text': 'text'}, 141: {'filename': 'cell_56.png', 'text': 'text'}, 142: {'filename': 'cell_57.png', 'text': '6'}, 143: {'filename': 'cell_58.png', 'text': 'text'}, 144: {'filename': 'cell_59.png', 'text': 'text'}, 146: {'filename': 'cell_60.png', 'text': '1'}, 147: {'filename': 'cell_61.png', 'text': 'text'}, 148: {'filename': 'cell_62.png', 'text': 'text'}, 149: {'filename': 'cell_63.png', 'text': 'text'}, 150: {'filename': 'cell_64.png', 'text': '9'}, 151: {'filename': 'cell_65.png', 'text': 'text'}, 152: {'filename': 'cell_66.png', 'text': 'text'}, 153: {'filename': 'cell_67.png', 'text': 'text'}, 154: {'filename': 'cell_68.png', 'text': 'text'}, 155: {'filename': 'cell_69.png', 'text': 'text'}, 157: {'filename': 'cell_70.png', 'text': 'text'}, 158: {'filename': 'cell_71.png', 'text': '10'}, 159: {'filename': 'cell_72.png', 'text': 'text'}, 160: {'filename': 'cell_73.png', 'text': '70'}, 161: {'filename': 'cell_74.png', 'text': 'total'}, 162: {'filename': 'cell_75.png', 'text': 'total'}, 163: {'filename': 'cell_76.png', 'text': 'text'}, 164: {'filename': 'cell_77.png', 'text': 'text'}, 165: {'filename': 'cell_78.png', 'text': '11'}, 166: {'filename': 'cell_79.png', 'text': 'text'}, 168: {'filename': 'cell_80.png', 'text': 'text'}, 169: {'filename': 'cell_81.png', 'text': 'text'}, 170: {'filename': 'cell_82.png', 'text': 'text'}, 171: {'filename': 'cell_83.png', 'text': 'text'}, 172: {'filename': 'cell_84.png', 'text': 'text'}, 173: {'filename': 'cell_85.png', 'text': '12'}, 174: {'filename': 'cell_86.png', 'text': 'text'}, 175: {'filename': 'cell_87.png', 'text': 'text'}, 176: {'filename': 'cell_88.png', 'text': 'text'}, 177: {'filename': 'cell_89.png', 'text': 'text'}, 179: {'filename': 'cell_90.png', 'text': 'text'}, 180: {'filename': 'cell_91.png', 'text': 'text'}, 181: {'filename': 'cell_92.png', 'text': '13'}, 182: {'filename': 'cell_93.png', 'text': 'text'}, 183: {'filename': 'cell_94.png', 'text': 'text'}, 184: {'filename': 'cell_95.png', 'text': 'text'}, 185: {'filename': 'cell_96.png', 'text': 'text'}, 186: {'filename': 'cell_97.png', 'text': 'text'}, 187: {'filename': 'cell_98.png', 'text': 'text'}, 188: {'filename': 'cell_99.png', 'text': '14'}, 2: {'filename': 'cell_100.png', 'text': 'text'}, 3: {'filename': 'cell_101.png', 'text': 'text'}, 4: {'filename': 'cell_102.png', 'text': 'text'}, 5: {'filename': 'cell_103.png', 'text': 'text'}, 6: {'filename': 'cell_104.png', 'text': 'text'}, 7: {'filename': 'cell_105.png', 'text': 'text'}, 8: {'filename': 'cell_106.png', 'text': '15'}, 9: {'filename': 'cell_107.png', 'text': 'text'}, 10: {'filename': 'cell_108.png', 'text': 'text'}, 11: {'filename': 'cell_109.png', 'text': 'text'}, 13: {'filename': 'cell_110.png', 'text': 'text'}, 14: {'filename': 'cell_111.png', 'text': 'text'}, 15: {'filename': 'cell_112.png', 'text': 'text'}, 16: {'filename': 'cell_113.png', 'text': '16'}, 17: {'filename': 'cell_114.png', 'text': 'text'}, 18: {'filename': 'cell_115.png', 'text': 'text'}, 19: {'filename': 'cell_116.png', 'text': 'text'}, 20: {'filename': 'cell_117.png', 'text': 'text'}, 21: {'filename': 'cell_118.png', 'text': 'text'}, 22: {'filename': 'cell_119.png', 'text': 'text'}, 24: {'filename': 'cell_120.png', 'text': '17'}, 25: {'filename': 'cell_121.png', 'text': '1'}, 26: {'filename': 'cell_122.png', 'text': 'text'}, 27: {'filename': 'cell_123.png', 'text': 'text'}, 28: {'filename': 'cell_124.png', 'text': 'text'}, 29: {'filename': 'cell_125.png', 'text': 'text'}, 30: {'filename': 'cell_126.png', 'text': 'text'}, 31: {'filename': 'cell_127.png', 'text': '18'}, 32: {'filename': 'cell_128.png', 'text': 'text'}, 33: {'filename': 'cell_129.png', 'text': 'text'}, 35: {'filename': 'cell_130.png', 'text': 'text'}, 36: {'filename': 'cell_131.png', 'text': 'text'}, 37: {'filename': 'cell_132.png', 'text': 'text'}, 38: {'filename': 'cell_133.png', 'text': 'text'}, 39: {'filename': 'cell_134.png', 'text': '18'}, 40: {'filename': 'cell_135.png', 'text': 'text'}, 41: {'filename': 'cell_136.png', 'text': 'text'}, 42: {'filename': 'cell_137.png', 'text': 'text'}, 43: {'filename': 'cell_138.png', 'text': 'text'}, 44: {'filename': 'cell_139.png', 'text': 'text'}, 46: {'filename': 'cell_140.png', 'text': 'text'}, 47: {'filename': 'cell_141.png', 'text': '20'}, 48: {'filename': 'cell_142.png', 'text': 'text'}, 49: {'filename': 'cell_143.png', 'text': 'text'}, 50: {'filename': 'cell_144.png', 'text': 'text'}, 51: {'filename': 'cell_145.png', 'text': 'text'}, 52: {'filename': 'cell_146.png', 'text': 'text'}, 53: {'filename': 'cell_147.png', 'text': 'text'}, 54: {'filename': 'cell_148.png', 'text': '21'}, 55: {'filename': 'cell_149.png', 'text': 'text'}, 57: {'filename': 'cell_150.png', 'text': 'text'}, 58: {'filename': 'cell_151.png', 'text': 'text'}, 59: {'filename': 'cell_152.png', 'text': 'text'}, 60: {'filename': 'cell_153.png', 'text': 'text'}, 61: {'filename': 'cell_154.png', 'text': 'text'}, 62: {'filename': 'cell_155.png', 'text': '22'}, 63: {'filename': 'cell_156.png', 'text': 'text'}, 64: {'filename': 'cell_157.png', 'text': 'text'}, 65: {'filename': 'cell_158.png', 'text': 'text'}, 66: {'filename': 'cell_159.png', 'text': 'text'}, 68: {'filename': 'cell_160.png', 'text': 'text'}, 69: {'filename': 'cell_161.png', 'text': 'text'}, 70: {'filename': 'cell_162.png', 'text': '23'}, 71: {'filename': 'cell_163.png', 'text': 'text'}, 72: {'filename': 'cell_164.png', 'text': 'text'}, 73: {'filename': 'cell_165.png', 'text': 'text'}, 74: {'filename': 'cell_166.png', 'text': 'text'}, 75: {'filename': 'cell_167.png', 'text': 'text'}, 76: {'filename': 'cell_168.png', 'text': 'text'}, 77: {'filename': 'cell_169.png', 'text': '24'}, 79: {'filename': 'cell_170.png', 'text': 'text'}, 80: {'filename': 'cell_171.png', 'text': 'text'}, 81: {'filename': 'cell_172.png', 'text': 'text'}, 82: {'filename': 'cell_173.png', 'text': 'text'}, 83: {'filename': 'cell_174.png', 'text': 'text'}, 84: {'filename': 'cell_175.png', 'text': 'text'}, 85: {'filename': 'cell_176.png', 'text': '25'}, 86: {'filename': 'cell_177.png', 'text': 'text'}, 87: {'filename': 'cell_178.png', 'text': 'text'}, 88: {'filename': 'cell_179.png', 'text': 'text'}, 90: {'filename': 'cell_180.png', 'text': 'text'}, 91: {'filename': 'cell_181.png', 'text': 'text'}, 92: {'filename': 'cell_182.png', 'text': 'text'}, 93: {'filename': 'cell_183.png', 'text': '26'}, 94: {'filename': 'cell_184.png', 'text': 'text'}, 95: {'filename': 'cell_185.png', 'text': 'text'}, 96: {'filename': 'cell_186.png', 'text': 'text'}, 97: {'filename': 'cell_187.png', 'text': 'text'}, 98: {'filename': 'cell_188.png', 'text': 'text'}, 99: {'filename': 'cell_189.png', 'text': 'text'}})\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "image_filenames = os.listdir(folder_path)\n",
    "sorted_detected_text = OrderedDict(natsorted(detected_text.items(), key=lambda item: item[1]['filename'])) #sort dictionary items on the basis of filename \n",
    "print(sorted_detected_text)\n",
    "print(len(sorted_detected_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add the result to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S.No., Plate ID, Thick, Width, Length, Unique ID, Quality Remark, \n",
      "1, 3419411ACC, 10, 2000, 12000, E250BR, 04R, \n",
      "2, 3419411ACE, 10, 2000, 12000, text, 04, \n",
      "3, 3419409CCB, 10, 2000, 12000, text, 04k, \n",
      "4, 3419409CCC, 10, 2000, 12000, text, 04R, \n",
      "5, 3419432DAO, 22, 2000, 12000, text, 01pcs, \n",
      "6, text, text, text, text, text, text, \n",
      "7, text, text, 1, text, text, text, \n",
      "6, text, text, 1, text, text, text, \n",
      "9, text, text, text, text, text, text, \n",
      "10, text, 70, total, total, text, text, \n",
      "11, text, text, text, text, text, text, \n",
      "12, text, text, text, text, text, text, \n",
      "13, text, text, text, text, text, text, \n",
      "14, text, text, text, text, text, text, \n",
      "15, text, text, text, text, text, text, \n",
      "16, text, text, text, text, text, text, \n",
      "17, 1, text, text, text, text, text, \n",
      "18, text, text, text, text, text, text, \n",
      "18, text, text, text, text, text, text, \n",
      "20, text, text, text, text, text, text, \n",
      "21, text, text, text, text, text, text, \n",
      "22, text, text, text, text, text, text, \n",
      "23, text, text, text, text, text, text, \n",
      "24, text, text, text, text, text, text, \n",
      "25, text, text, text, text, text, text, \n",
      "26, text, text, text, text, text, text, \n",
      "Data has been written to D:\\handwritten_recognition\\data26.xlsx\n"
     ]
    }
   ],
   "source": [
    "excel_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "excel_file_path = fr'D:\\handwritten_recognition\\{excel_name}.xlsx' # Specify your desired file path\n",
    "\n",
    "rows = []\n",
    "row = []\n",
    "i = 0\n",
    "for key, value in sorted_detected_text.items():\n",
    "    i+=1\n",
    "    row.append(value['text']) # append different cells of a single row \n",
    "    print(f'{value['text']},',end=' ')\n",
    "    if i%7==0:\n",
    "        print()\n",
    "        rows.append(row)  # append the collected row to rows\n",
    "        row = [] \n",
    "# append any remaining items in the last row (if less than 7)\n",
    "if row:\n",
    "    rows.append(row)\n",
    "\n",
    "# create DataFrame and save to Excel\n",
    "df = pd.DataFrame(rows,columns=rows[0]) # add first row as column name\n",
    "df = df[1:] # removing the first as we used it as column\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# save the DataFrame to Excel\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f'Data has been written to {excel_file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
